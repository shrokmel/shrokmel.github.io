---
layout: post
title: "Distance Calculation"
excerpt: "Efficiently calculating distances between particles using Python."
categories: articles
author: arvind_ravichandran
tags: [python, distance, optimisation]
comments: true
share: true
modified: 2017-04-23T02:32:14-04:00
---

Time complexity of an algorithm quantifies the amount of time taken for a calculation as a function of the quantity of input. Naively speaking, most molecular dynamics algorithms are of quadratic time complexity, or O(n<sup>2</sup>) problems. This means that as the number of entities or particles, _n_, in the system increases, the time taken to complete the calculations increases quadratically. 

Molecular dynamics algorithms are O(n<sup>2</sup>) because computing distances between particles, when dealt with naively, is of quadratic time complexity. This is also the bottle neck in most algorithms. By way of neighbour lists and cell lists, it is indeed possible to make this logarithmic, but for the purpose of this post, let's look at tricks to optimise the simple approach using python.

This problem can be most efficiently solved by simply using the ```scipy.spatial.distance.pdist``` function. But this post will help in understanding how to approach this _type_ of problems using Python. For instance, the issue at hand might not always be computing the distance between particles. The problem could be computing the dot product of orientations of all combinations of particle pairs. With this in mind, let us begin! 

## Particles in open boundaries

Imagine that you have a system with open boundaries in two dimensions, with _N=100_ particles. 

<figure>
<a href="/sandbox/particles.png"><img src="/sandbox/particles.png" alt="image"></a>
<figcaption>Randomly generated positions of 100 particles. </figcaption>
</figure>

Our goal is to compute the forces (it doesn't matter what sort of force) between all these particles at a given time step. With the forces, we can calculate their accelerations, from which we obtain velocities and then their displacements. The displacements will tell us their positions in the next time step. We use this to compute the forces and the cycle goes on, for some period of time.

Assuming that these forces are functions of distances, we need to compute the distance between all _permutations_ of particles. We can construct a matrix of distances, which will look like this:

\\[
\begin{pmatrix} 
d_{1,1}     & d_{1,2} & .. & d_{1,N} \\\
d_{2,1}     & d_{2,2} & .. & d_{2,N} \\\ 
..          & ..      & .. & ..      \\\
d_{N,1}     & d_{N,2} & .. & d_{N,N} \\\
\end{pmatrix}
\\]


We must generate the initial positions of 100 particles, and construct the distance matrix:

```python
import numpy as np

L   = 100       # simulation box dimension
N   = 100       # Number of particles
dim = 2         # Dimensions

# Generate random positions of particles
r = (np.random.random(size=(N,dim))-0.5)*L

# Compute distance matrix
D = np.zeros(shape=(N,N), dtype=float)

# This is the N-squared operation
for i in range(N):
    for j in range(N):
        dr = r[j]-r[i]                  # difference between 2 positions
        D[i,j] = np.sqrt(sum(dr*dr))    # calculate distance and store

print(D[:3,:3])                         # print small section of the matrix

>>> [[  0.          14.47476649  68.4285819 ]
    [ 14.47476649   0.          53.99224333]
    [ 68.4285819   53.99224333   0.        ]]
```
#### Upper Triangular Distance Matrix

For 100 particles, in this algorithm, we are making 10,000 distance calculations. We can do better. Firstly, notice that the diagonal values are zero. The distance of a particle with itself is always obviously zero. Secondly, the matrix is symmetric _i.e._ reversing the order of indices does not affect the calculated distance. So we are computing distances twice, when we can get away with half the number of calculations. We can halve the number of calculations by simply computing the _upper triangular_ matrix of _D_. The N-squared operation will then become:

##### Code 1
```python
# This is the N-squared operation
for i in range(N):
    for j in range(i+1,N):              # j>i (second index is always greater than first)
        dr = r[j]-r[i]                  # difference between 2 positions
        D[i,j] = np.sqrt(sum(dr*dr))    # calculate distance and store
```

We have now decreased the number of calculations to 4,950. In terms of the algorithm, there isn't much more that we can do. However, we can do a lot better if we learn some pythonic tricks. 

* Perform expensive numpy functions such as, ```np.sqrt```, as few times as possible. For instance, the solution will be identical, if we take the square root of the matrix, after the squared distances are calculated.
* As much as possible avoid explicit for loops. They are slow in Python. If you find a way to offload looping duties to Python implicity, you will generally get much more readable and faster code.
* Reduce data access. So far, we created an array of zeros, and updated their values by accessing them. This is inefficient.


With just these three things in mind, let's do something better! For this we need to learn a neat numpy function called ```np.triu_indices```. This gives the upper triangular matrix indices _i.e._ exactly the same indices that we were generating using our range function in a for loop. 

##### Code 2
```python
import numpy as np

L   = 100       # simulation box dimension
N   = 100       # Number of particles
dim = 2         # Dimensions

# Generate random positions of particles
r = (np.random.random(size=(N,dim))-0.5)*L

# uti is a list of two (1-D) numpy arrays  
# containing the indices of the upper triangular matrix
uti = np.triu_indices(100,k=1)        # k=1 eliminates diagonal indices

# uti[0] is i, and uti[1] is j from the previous example 
dr = r[uti[0]] - r[uti[1]]            # computes differences between particle positions
D = np.sqrt(np.sum(dr*dr, axis=1))    # computes distances; D is a 4950 x 1 np array
```

We have done in three lines, what we previously achieved in five, and we have no more for loops. Timing them on my computer shows the gulf of performance that we have achieved for 1000 particles: Code 1 takes around 3.4 seconds, and Code 2 takes around 0.0005 seconds. Perhaps you noticed that I did cheat a little by not generating a _NxN_ matrix, as in Code 1. But even recasting the values using ```scipy.spatial.distance.squareform``` into a _NxN_ matrix does not slow the code significantly.

The easiest method to solve this problem, as I mentined earlier, is to use the ```scipy.spatial.distance.pdist``` function. Its a one-liner: 

##### Code 3
```python
import numpy as np
from scipy.spatial.distance import pdist

L   = 100       # simulation box dimension
N   = 100       # Number of particles
dim = 2         # Dimensions

# Generate random positions of particles
r = (np.random.random(size=(N,dim))-0.5)*L
D = pdist(r)
```

However, to my surprise, I found that this was slower than Code 2.  


